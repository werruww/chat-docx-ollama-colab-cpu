# -*- coding: utf-8 -*-
"""suc_ChatlamaChatwithOllamaModels.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mcRWT-vyGqpfFFe7DSw6sa-tdYaY26uH
"""









# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/codersbranch/Chatlama.git
# %cd Chatlama

!pip install -r requirements.txt

curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &

!ollama pull llama3.2

streamlit run app.py

!pip install pyngrok




from pyngrok import ngrok
import threading
import time

ngrok_token = "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(3000).public_url
    print(f"ðŸš€ Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!node hello-world.js

!pip install pyngrok

# Commented out IPython magic to ensure Python compatibility.
!nohup ollama serve &
# %cd /content/Chatlama
from pyngrok import ngrok
import threading
import time

ngrok_token = "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(8501).public_url
    print(f"ðŸš€ Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!streamlit run app.py

